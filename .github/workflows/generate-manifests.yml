name: Generate Image Manifests & Pre-cache GitHub API

# Runs on every push to main, and on a schedule every 6 hours to keep
# cached GitHub API data fresh (avoids rate-limit issues for visitors).
on:
  push:
    branches: [main]
  schedule:
    # Every 6 hours  (UTC)
    - cron: "0 */6 * * *"
  workflow_dispatch:

jobs:
  generate-manifests:
    runs-on: ubuntu-latest
    permissions:
      contents: write   # needed to commit the generated manifest files back

    steps:
      # ── 1. Checkout ────────────────────────────────────────────────────────
      - name: Checkout repository
        uses: actions/checkout@v4

      # ── 2. Generate image manifests ────────────────────────────────────────
      # For every directory under public/assets/features/ and
      # public/assets/addons/<id>/ we create a manifest.json listing all image
      # files found there. The site's JS reads these manifests at runtime
      # instead of hitting the GitHub Contents API (which is rate-limited).
      - name: Generate image manifests
        run: |
          python3 << 'PYEOF'
          import os, json

          IMAGE_EXTENSIONS = {".jpg", ".jpeg", ".png", ".webp", ".gif"}

          def generate_manifest(directory):
              if not os.path.isdir(directory):
                  return
              images = sorted([
                  f for f in os.listdir(directory)
                  if os.path.splitext(f)[1].lower() in IMAGE_EXTENSIONS
              ])
              manifest_path = os.path.join(directory, "manifest.json")
              with open(manifest_path, "w") as fh:
                  json.dump({"images": images}, fh, indent=2)
              print(f"Generated {manifest_path} — {len(images)} image(s): {images}")

          # Feature image folders
          features_root = "public/assets/features"
          if os.path.isdir(features_root):
              for entry in os.listdir(features_root):
                  path = os.path.join(features_root, entry)
                  if os.path.isdir(path):
                      generate_manifest(path)

          # Addon image folders (each addon sub-directory)
          addons_root = "public/assets/addons"
          if os.path.isdir(addons_root):
              for entry in os.listdir(addons_root):
                  path = os.path.join(addons_root, entry)
                  if os.path.isdir(path):
                      generate_manifest(path)
          PYEOF

      # ── 3. Pre-fetch GitHub API data ───────────────────────────────────────
      # We fetch contributor and commit data from the GitHub API using the
      # built-in GITHUB_TOKEN (no rate-limit for authenticated requests within
      # Actions) and bake the results into static JSON files under
      # public/api-cache/.  The site's JS will load these files first instead
      # of hitting the live API, and only falls back to the live API if the
      # cached files are missing.
      - name: Pre-fetch GitHub API data
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          mkdir -p public/api-cache

          # Helper: fetch JSON from GitHub API with auth
          fetch_api() {
            curl -sSf \
              -H "Accept: application/vnd.github+json" \
              -H "Authorization: Bearer ${GH_TOKEN}" \
              -H "X-GitHub-Api-Version: 2022-11-28" \
              "$1"
          }

          # Read repos from config.json
          PANEL_REPO=$(python3 -c "import json; d=json.load(open('config.json')); print(d['site']['githubPanel'])")
          DAEMON_REPO=$(python3 -c "import json; d=json.load(open('config.json')); print(d['site']['githubDaemon'])")

          echo "Panel repo:  $PANEL_REPO"
          echo "Daemon repo: $DAEMON_REPO"

          # Contributors
          fetch_api "https://api.github.com/repos/${PANEL_REPO}/contributors?per_page=100" \
            > public/api-cache/contributors-panel.json
          fetch_api "https://api.github.com/repos/${DAEMON_REPO}/contributors?per_page=100" \
            > public/api-cache/contributors-daemon.json

          # Recent commits (first page only, for the previews)
          fetch_api "https://api.github.com/repos/${PANEL_REPO}/commits?per_page=15" \
            > public/api-cache/commits-panel-p1.json
          fetch_api "https://api.github.com/repos/${DAEMON_REPO}/commits?per_page=15" \
            > public/api-cache/commits-daemon-p1.json

          # Repo metadata (for star counts)
          fetch_api "https://api.github.com/repos/${PANEL_REPO}" \
            > public/api-cache/repo-panel.json
          fetch_api "https://api.github.com/repos/${DAEMON_REPO}" \
            > public/api-cache/repo-daemon.json

          # Individual user profiles — fetch each unique contributor and save to
          # public/api-cache/user-<login>.json. The team section reads these files
          # first so it never hits the live API per-user (which causes rate limits fast).
          python3 << \'PYEOF\'
import json, subprocess, os, sys

def fetch(url):
    token = os.environ.get("GH_TOKEN", "")
    r = subprocess.run([
        "curl", "-sSf",
        "-H", "Accept: application/vnd.github+json",
        "-H", f"Authorization: Bearer {token}",
        "-H", "X-GitHub-Api-Version: 2022-11-28",
        url
    ], capture_output=True, text=True)
    return json.loads(r.stdout) if r.returncode == 0 else None

seen = set()
for fname in ["public/api-cache/contributors-panel.json",
              "public/api-cache/contributors-daemon.json"]:
    if not os.path.exists(fname): continue
    try: contributors = json.load(open(fname))
    except: continue
    if not isinstance(contributors, list): continue
    for c in contributors:
        login = c.get("login")
        if not login or login in seen: continue
        seen.add(login)
        data = fetch(f"https://api.github.com/users/{login}")
        if data:
            json.dump(data, open(f"public/api-cache/user-{login}.json", "w"))
            print(f"Cached: {login}")

print(f"Total cached: {len(seen)}")
\'PYEOF\'

          # Timestamp so the frontend knows how fresh the cache is
          python3 -c "import json, time; json.dump({'generated_at': int(time.time())}, open('public/api-cache/meta.json','w'))"

          echo "API cache files:"
          ls -lh public/api-cache/

      # ── 4. Commit changes back to repo ─────────────────────────────────────
      - name: Commit generated files
        run: |
          git config user.name  "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add public/assets/*/manifest.json \
                  public/assets/addons/*/manifest.json \
                  public/api-cache/ \
                  || true
          # Only commit if there are actual changes
          git diff --cached --quiet && echo "No changes to commit" || \
            git commit -m "chore: regenerate image manifests and api cache [skip ci]"
          git push || echo "Nothing to push"
